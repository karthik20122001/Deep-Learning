{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19BAI1076_B_KARTHIK_GPT2_TEXT_GENERATION.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"H7LoMj4GA4n_"},"source":["## **Train a GPT-2 model on a custom dataset**\n","\n","In this notebook I am taking a pretrained GPT-2 model and fine tune it on a custom datset. I have prepared a dataset(txt file) with combining all the deep learning lab reports I have done and put it into gpt2 model for training.\n","The result is pretty amazing\n"]},{"cell_type":"code","metadata":{"id":"KBkpRgBCBS2_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638293926197,"user_tz":-330,"elapsed":83307,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"e901472f-632e-4d96-fb6c-5b67c9fa5551"},"source":["%tensorflow_version 1.x\n","!pip install -q gpt-2-simple\n","import gpt_2_simple as gpt2\n","from datetime import datetime\n","from google.colab import files"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n","\u001b[K     |████████████████████████████████| 489.6 MB 22 kB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 34.6 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 25.0 MB/s \n","\u001b[K     |████████████████████████████████| 463 kB 45.1 MB/s \n","\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bj2IJLHP3KwE"},"source":["## GPU"]},{"cell_type":"code","metadata":{"id":"sUmTooTW3osf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638287208154,"user_tz":-330,"elapsed":307,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"dbe36a33-933a-4c86-bd6a-a01632faaa6f"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Nov 30 15:46:47 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"0wXB05bPDYxS"},"source":["## Downloading GPT-2\n"]},{"cell_type":"code","metadata":{"id":"P8wSlgXoDPCR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638293987600,"user_tz":-330,"elapsed":55196,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"32e07b78-5f2c-4883-f954-76c6ca1be9e3"},"source":["gpt2.download_gpt2(model_name=\"124M\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Fetching checkpoint: 1.05Mit [00:00, 170Mit/s]                                                      \n","Fetching encoder.json: 1.05Mit [00:00, 1.60Mit/s]\n","Fetching hparams.json: 1.05Mit [00:00, 570Mit/s]                                                    \n","Fetching model.ckpt.data-00000-of-00001: 498Mit [00:49, 10.0Mit/s]                                  \n","Fetching model.ckpt.index: 1.05Mit [00:00, 281Mit/s]                                                \n","Fetching model.ckpt.meta: 1.05Mit [00:00, 2.00Mit/s]\n","Fetching vocab.bpe: 1.05Mit [00:00, 1.94Mit/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"N8KXuKWzQSsN"},"source":["## Mounting Google Drive"]},{"cell_type":"code","metadata":{"id":"puq4iC6vUAHc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638294014433,"user_tz":-330,"elapsed":23612,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"22bd93f4-0c3f-4be3-b6dd-0fdb4eeea2e1"},"source":["gpt2.mount_gdrive()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"BT__brhBCvJu"},"source":["## Uploading a Text File to be Trained to Colaboratory"]},{"cell_type":"code","metadata":{"id":"6OFnPCLADfll"},"source":["file_name = \"DeepLearningCorpus.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z6okFD8VKtS"},"source":["gpt2.copy_file_from_gdrive(file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdpZQXknFNY3"},"source":["## Finetune GPT-2\n","\n","The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n","\n","The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n","\n","\n","\n","Other optional-but-helpful parameters for `gpt2.finetune`:\n","\n","\n","*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n","* **`sample_every`**: Number of steps to print example output\n","* **`print_every`**: Number of steps to print training progress.\n","* **`learning_rate`**:  Learning rate for the training.\n","*  **`run_name`**: subfolder within `checkpoint` to save the model.\n","* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "]},{"cell_type":"code","metadata":{"id":"aeXshJM-Cuaf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638293100932,"user_tz":-330,"elapsed":5622618,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"bd77a67e-7ee2-46a7-d534-adafc8f98240"},"source":["sess = gpt2.start_tf_sess()\n","\n","gpt2.finetune(sess,\n","              dataset=file_name,\n","              model_name='124M',\n","              steps=1000,\n","              restore_from='fresh',\n","              run_name='run1',\n","              print_every=10,\n","              sample_every=200,\n","              save_every=500\n","              )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading checkpoint models/124M/model.ckpt\n","INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n","Loading dataset...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 787.07it/s]"]},{"output_type":"stream","name":"stdout","text":["dataset has 5528 tokens\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[10 | 60.83] loss=2.07 avg=2.07\n","[20 | 115.93] loss=0.99 avg=1.53\n","[30 | 170.99] loss=0.27 avg=1.11\n","[40 | 226.02] loss=0.08 avg=0.84\n","[50 | 281.01] loss=0.13 avg=0.70\n","[60 | 336.01] loss=0.03 avg=0.58\n","[70 | 391.07] loss=0.07 avg=0.51\n","[80 | 446.13] loss=0.04 avg=0.45\n","[90 | 501.20] loss=0.03 avg=0.40\n","[100 | 556.26] loss=0.02 avg=0.36\n","[110 | 611.29] loss=0.02 avg=0.33\n","[120 | 666.37] loss=0.02 avg=0.30\n","[130 | 721.43] loss=0.02 avg=0.28\n","[140 | 776.48] loss=0.02 avg=0.26\n","[150 | 831.55] loss=0.02 avg=0.24\n","[160 | 886.60] loss=0.02 avg=0.23\n","[170 | 941.64] loss=0.02 avg=0.21\n","[180 | 996.68] loss=0.02 avg=0.20\n","[190 | 1051.74] loss=0.02 avg=0.19\n","[200 | 1106.79] loss=0.01 avg=0.18\n","======== SAMPLE 1 ========\n"," labelled the region proposal algorithms \n","\n","Index Terms—Transfer Learning, EfficientNet, ImageNet I. INTRODUCTION \n","Serious complications can occur as a result of mal- positioned lines and tubes in patients. Doctors and nurses fre- quently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. \n","Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube mal-positioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of mal-positioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. \n","Earlier detection of mal-positioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","Our objective is to apply transfer learning to detect the abnormal position of the catheters and tubes. There are several pretrained models like VGGNet, ResNet, Xception, Inception, \n","Densenet, Efficientnet, MobileNet, etc. In the experiment we will be using the Efficientnet model. The Efficientnet model is built on top of the popular imagenet dataset. \n","We load the model with a dropout rate of 0.5 and we apply GlobalAveragePooling2D. Finally we add a dense layer with sigmoid activation function to predict our labels. \n","\n","III. DATASET \n","The data we used for our study was provided on the Kaggle website. There were 40,000 images to categorize a tube that is poorly placed. The train and test set contains image IDs, binary labels, and patient IDs. \n","\n","V. RESULTS \n","The results were quit astonishing. The following image shows the auc score on the train and test set after 20 epochs. \n","\n","Proceeding further the submission score (auc score on the hidden test set) the score was improved from 0.7181 to 0.77159 \n","Why EfficientNet model performs better? \n","The term ‘Efficient’ in Efficient Net strongly suggests that this convolutional neural network is the next state-of-the-art network which not only has less number of parameters but also the winner of ILSVRC-2019 with 84.4% and 97.1% as the top-1 and top-5 accuracy respectively.  \n","V. RESULTS \n","\n","The results were quit astonishing. The following image shows the auc score on the train and test set after 20 epochs. \n","Proceeding further the submission score (auc score on the hidden test set) the score was improved from 0.7181 to 0.77159 \n","\n","VI. CONCLUSION \n","We observed that there is a significant improvement in the density of objects in our image. This can be seen in the percentage of normal objects in the image. We saw that the object density was getting better the more we trained. After 20 epochs the auc score on Train set was 0.9249 and that on Test set was 0.9 but auc for Test set was 0.77159. In the upcoming studies we will look to solve this problem. \n","\n","Blood Cell Detection: Faster-RCNN \n","\n","Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolu- tional features with the detection network, thus enabling nearly cost-free region proposals. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the\n","\n","[210 | 1184.80] loss=0.02 avg=0.17\n","[220 | 1239.85] loss=0.02 avg=0.16\n","[230 | 1294.90] loss=0.01 avg=0.16\n","[240 | 1349.95] loss=0.01 avg=0.15\n","[250 | 1404.98] loss=0.02 avg=0.14\n","[260 | 1460.01] loss=0.02 avg=0.14\n","[270 | 1515.04] loss=0.02 avg=0.13\n","[280 | 1570.04] loss=0.02 avg=0.13\n","[290 | 1625.04] loss=0.02 avg=0.12\n","[300 | 1680.05] loss=0.01 avg=0.12\n","[310 | 1735.06] loss=0.01 avg=0.12\n","[320 | 1790.08] loss=0.01 avg=0.11\n","[330 | 1845.09] loss=0.01 avg=0.11\n","[340 | 1900.12] loss=0.01 avg=0.11\n","[350 | 1955.11] loss=0.02 avg=0.10\n","[360 | 2010.14] loss=0.02 avg=0.10\n","[370 | 2065.16] loss=0.01 avg=0.10\n","[380 | 2120.20] loss=0.01 avg=0.09\n","[390 | 2175.24] loss=0.02 avg=0.09\n","[400 | 2230.28] loss=0.01 avg=0.09\n","======== SAMPLE 1 ========\n","-\n","\n","From the above results we can observe that the logistic regression model is performing poorly. This is expected as we are using only a Single layer and single node. We are still in a early stage. In the upcoming lab experiments we can see how we can improve the performance by increasing the no. of layers, no. of nodes in a layer, choosing activation functions, etc. In the 2 two layer model we can see a significant increase in the accuracy when compared to the previous one. \n","Finally the 7 layer model after fine-tuning the hyper param- eters gives the best performance with an accuracy of 93.29% \n","\n","VI. CONCLUSION \n","We observed that L-Layer model proved to be the best algo- rithm for the Titanic classification problem since the accuracy of L-Layer model is the highest and the false discovery rate is the lowest as compared to all other implemented algorithms. We also determined the features that were the most significant for the prediction. Pclass, sex, age, children and SibSp are the features that are correlated to the survival of the passengers. \n","\n","Catheter and Line Position Challenge Fine-tuning Hyperparameter with Keras \n","\n","Abstract—In this project, we can detect the presence and position of catheters and lines on chest x-rays. Use machine learning and deep learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. \n","\n","Index Terms—Hyperparameter, Tensorflow, Keras,CNN I. INTRODUCTION \n","Serious complications can occur as a result of mal- positioned lines and tubes in patients. Doctors and nurses fre- quently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. \n","Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube mal-positioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of mal-positioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. \n","Earlier detection of mal-positioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","We will be first implementing a simple CNN model with 11 layers. \n","\n","Our main objective is to tune the hyperparameters such as the learning rate, batch size, no of epochs, no of hidden units, no of layers. \n","For that we will be using the Keras tuner library. The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called hyperparameter tuning or hypertuning. \n","Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types: \n","Model hyperparameters which influence model selection such as the number and width of hidden layers Algorithm hyperparameters which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) \n","\n","III. DATASET \n","The data we used for our study was provided on the Kaggle website. There were 40,000 images to categorize a tube that is poorly placed. The train and test set contains image IDs, binary labels, and patient IDs. \n","\n","V. RESULTS\n","\n","[410 | 2307.05] loss=0.01 avg=0.09\n","[420 | 2362.08] loss=0.01 avg=0.09\n","[430 | 2417.11] loss=0.01 avg=0.08\n","[440 | 2472.13] loss=0.02 avg=0.08\n","[450 | 2527.13] loss=0.01 avg=0.08\n","[460 | 2582.17] loss=0.01 avg=0.08\n","[470 | 2637.21] loss=0.01 avg=0.08\n","[480 | 2692.24] loss=0.01 avg=0.07\n","[490 | 2747.24] loss=0.01 avg=0.07\n","[500 | 2802.25] loss=0.01 avg=0.07\n","Saving checkpoint/run1/model-500\n","[510 | 2860.16] loss=0.01 avg=0.07\n","[520 | 2915.17] loss=0.01 avg=0.07\n","[530 | 2970.16] loss=0.01 avg=0.07\n","[540 | 3025.18] loss=0.01 avg=0.07\n","[550 | 3080.23] loss=0.01 avg=0.06\n","[560 | 3135.27] loss=0.01 avg=0.06\n","[570 | 3190.33] loss=0.02 avg=0.06\n","[580 | 3245.39] loss=0.01 avg=0.06\n","[590 | 3300.44] loss=0.01 avg=0.06\n","[600 | 3355.49] loss=0.01 avg=0.06\n","======== SAMPLE 1 ========\n","ways, and if any discrepancy is found in their blood, actions can be taken quickly to diagnose that.  Manually looking at the sample via a microscope is a tedious process. And this is where Deep Learning models play such a vital role. They can classify and detect the blood cells from microscopic images with impressive precision. \n","\n","V. RESULTS \n","So our model has been trained and the weights are set. Keras_frcnn makes the predictions for the new images and saves them in a new folder. We just have to run the test_frcnn.py file to save the images. Below are a few examples of the predictions I got after implementing Faster R-CNN: \n","\n","VI. CONCLUSION \n","R-CNN algorithms have truly been a game-changer for object detection tasks. There has suddenly been a spike in recent years in the amount of computer vision applications being created, and R-CNN is at the heart of most of them. Keras_frcnn proved to be an excellent library for object detection, and in the upcoming labs, we will focus on more advanced techniques like YOLO, SSD, etc. \n","\n","Realtime Object Detection in Video YOLOv3 \n","\n","Abstract—Object detection using deep learning has achieved very good performance but there are many problems with images in real-world shooting such as noise, blurring or rotating jitter, etc. These problems have a great impact on object detection. The main objective is to detect objects using You Only Look Once (YOLO) approach. The YOLO method has several advantages as compared to other object detection algorithms. In other algorithms like Convolutional Neural Network (CNN), Fast- Convolutional Neural Network the algorithm will not look at the image completely, but in YOLO ,the algorithm looks the image completely by predicting the bounding boxes using convolutional network and finds class probabilities for these boxes and also detectsthe image faster as compared to other algorithms. We have used this algorithm for detecting different types of objects and have created an android application which would return voice feedback to the user. \n","\n","Index Terms—YOLOv3, Object detection, Bounding boxes I. \n","\n","INTRODUCTION \n","Object detection is applied in numerous views, for example, mechanized vehicle frameworks, movement acknowledgment, a person on foot recognition, apply autonomy, robotized CCTV, object checking, etc. As of late, object recognition in light of profound learning has grown significantly. Basic target location techniques are separated into 2 species. They are recognition methodologies good with the locale proposition and single-step indicator[1]. YOLOv3 (seen just once) has a place with a solitary advance identifier. It is a quick and well- identified article location innovation. \n","Contrasted with quicker RCNNs and SSDs, YOLOv3 has a lower identification exactness than quicker R-CNN on little targets, however, the recognition speed is a lot quicker and can be utilized better for building. Simultaneously, the identi- fication precision of YOLOv3 resembles RCNN quicker when the objectives are not little. YOLOv3 is likewise better than SSD regarding location speed and exactness. In any case, the technique for getting the identification model via preparing an enormous number of tests is especially dictated by the huge number of tests. There are many methods for detecting an object, such as three-dimensional detection and digital image processing. \n","These algorithms are not tested with degraded images, i.e. they are trained with academic data sets, including ImageNet, COCO and VOC, etc. but they are not well tested with ran- domly captured data sets.The main issues ofimages captured in the real scene are: 1) Due to the instability of the camera, the captured images may be blurred. 2) The images can also \n","not be clear enough because the object can be obstructed. 3) The images may have poor quality as a result of bad weather, overexposure or low resolution. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","\n","YOLO (\"you only look once\") is one of the popular al- gorithm because it achieves high accuracy along with being able to run in real-time. The algorithm \"only looks once\" at the image, i.e. it requires only one forward propagation pass through the network so that it can make predictions. After non- max suppression, it gives the name of the recognized object along with the bounding boxes around them. \n","The YOLOv3 algorithm first separates an image into a grid. Each grid cell predicts some number of boundary boxes (sometimes referred to as anchor boxes) around objects that score highly with the aforementioned predefined classes. \n","Each boundary box has a respective confidence score of how accurate it assumes that\n","\n","[610 | 3432.46] loss=0.01 avg=0.06\n","[620 | 3487.53] loss=0.01 avg=0.06\n","[630 | 3542.59] loss=0.01 avg=0.05\n","[640 | 3597.65] loss=0.01 avg=0.05\n","[650 | 3652.70] loss=0.01 avg=0.05\n","[660 | 3707.78] loss=0.01 avg=0.05\n","[670 | 3762.84] loss=0.01 avg=0.05\n","[680 | 3817.86] loss=0.01 avg=0.05\n","[690 | 3872.90] loss=0.01 avg=0.05\n","[700 | 3927.96] loss=0.01 avg=0.05\n","[710 | 3983.02] loss=0.01 avg=0.05\n","[720 | 4038.08] loss=0.01 avg=0.05\n","[730 | 4093.14] loss=0.01 avg=0.05\n","[740 | 4148.16] loss=0.01 avg=0.05\n","[750 | 4203.21] loss=0.01 avg=0.05\n","[760 | 4258.22] loss=0.01 avg=0.04\n","[770 | 4313.23] loss=0.01 avg=0.04\n","[780 | 4368.24] loss=0.01 avg=0.04\n","[790 | 4423.23] loss=0.01 avg=0.04\n","[800 | 4478.23] loss=0.01 avg=0.04\n","======== SAMPLE 1 ========\n"," the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of mal-positioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines. \n","Earlier detection of mal-positioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","We will be first implementing a simple CNN model with 11 layers. \n","\n","Our main objective is to tune the hyperparameters such as the learning rate, batch size, no of epochs, no of hidden units, no of layers. \n","For that we will be using the Keras tuner library. The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called hyperparameter tuning or hypertuning. \n","Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types: \n","Model hyperparameters which influence model selection such as the number and width of hidden layers Algorithm hyperparameters which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) \n","\n","III. DATASET \n","The data we used for our study was provided on the Kaggle website. There were 40,000 images to categorize a tube that is poorly placed. The train and test set contains image IDs, binary labels, and patient IDs. \n","\n","V. RESULTS \n","After running the hyperparameter tuning for 10 epochs we got 0.0001 as the best learning rate for the model. \n","Proceeding further the submission score (auc score on the hidden test set) the score was improved from 0.68692 to 0.7181 \n","\n","VI. CONCLUSION \n","We observed that the hyperparameter tuning gives a signifi- cant boost to our model. I have applied hyperparameter tuning \n","only for the learning rate. In near future I will work on other hyperparameters like no. of layers, no. of hidden units, filter- size, etc as well to yield better results. And further we will look on implementing transfer learning as a solution to the problem. \n","\n","Catheter and Line Position Challenge Transfer Learning \n","\n","Abstract—In this project, we can detect the presence and position of catheters and lines on chest x-rays. Use machine learning and deep learning to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. \n","\n","Index Terms—Transfer Learning, EfficientNet, ImageNet I. INTRODUCTION \n","Serious complications can occur as a result of mal- positioned lines and tubes in patients. Doctors and nurses fre- quently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. \n","Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube mal-positioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of mal-positioned tubes is the\n","\n","[810 | 4554.88] loss=0.01 avg=0.04\n","[820 | 4609.91] loss=0.01 avg=0.04\n","[830 | 4664.93] loss=0.01 avg=0.04\n","[840 | 4719.94] loss=0.01 avg=0.04\n","[850 | 4774.95] loss=0.01 avg=0.04\n","[860 | 4829.97] loss=0.01 avg=0.04\n","[870 | 4884.98] loss=0.01 avg=0.04\n","[880 | 4940.02] loss=0.01 avg=0.04\n","[890 | 4995.04] loss=0.01 avg=0.04\n","[900 | 5050.07] loss=0.01 avg=0.04\n","[910 | 5105.14] loss=0.01 avg=0.04\n","[920 | 5160.19] loss=0.01 avg=0.04\n","[930 | 5215.24] loss=0.01 avg=0.04\n","[940 | 5270.27] loss=0.01 avg=0.03\n","[950 | 5325.32] loss=0.01 avg=0.03\n","[960 | 5380.35] loss=0.01 avg=0.03\n","[970 | 5435.40] loss=0.01 avg=0.03\n","[980 | 5490.46] loss=0.01 avg=0.03\n","[990 | 5545.54] loss=0.01 avg=0.03\n","[1000 | 5600.62] loss=0.01 avg=0.03\n","Saving checkpoint/run1/model-1000\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1058: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"]}]},{"cell_type":"markdown","metadata":{"id":"IXSuTNERaw6K"},"source":["After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n","\n","The checkpoint folder is copied as a `.rar` compressed file; so that I can download it and uncompress it locally."]},{"cell_type":"code","metadata":{"id":"VHdTL8NDbAh3"},"source":["gpt2.copy_checkpoint_to_gdrive(run_name='run1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pel-uBULXO2L"},"source":["## Load a Trained Model Checkpoint\n","\n","Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."]},{"cell_type":"code","metadata":{"id":"DCcx5u7sbPTD","executionInfo":{"status":"ok","timestamp":1638294369948,"user_tz":-330,"elapsed":10334,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}}},"source":["gpt2.copy_checkpoint_from_gdrive(run_name='run1')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fxL77nvAMAX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638294381249,"user_tz":-330,"elapsed":5684,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"baf41825-1008-428b-d30a-79bb87e22157"},"source":["sess = gpt2.start_tf_sess()\n","gpt2.load_gpt2(sess, run_name='run1')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading checkpoint checkpoint/run1/model-1000\n","INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"]}]},{"cell_type":"markdown","metadata":{"id":"ClJwpF_ACONp"},"source":["## Generate Text From The Trained Model\n","\n","After trained the model or loaded a retrained model from checkpoint, we can now generate text. `generate` generates a single text from the loaded model."]},{"cell_type":"code","metadata":{"id":"4RNY6RBI9LmL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638294413048,"user_tz":-330,"elapsed":23336,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"7a687d38-55f8-4f27-dcca-5d53219dee2f"},"source":["gpt2.generate(sess, run_name='run1')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["From Net Neutrality Wiki\n","The first article about the topic was published in 2015. This article, the third and final one of a series to understand the fundamentals of current day object detection elaborates the technical details of the Faster R-CNN detection pipeline. For a review of its predecessors, check out these summaries: Regions with CNN (R-CNN) and Fast R-CNN. \n","In the R-CNN family of papers, the evolution between versions was usually in terms of computational efficiency (integrating the different training stages), reduction in test time, and improvement in performance (mAP). These net- works usually consist of — a) A region proposal algorithm to generate “bounding boxes” or locations of possible objects in the image; b) A feature generation stage to obtain features of these objects, usually using a CNN; c) A classification layer to predict which class this object belongs to; and d) A regression layer to make the coordinates of the object bounding box more precise. \n","The only stand-alone portion of the network left in Fast R-CNN was the region proposal algorithm. Both R-CNN and Fast R-CNN use CPU based region proposal algorithms, Eg- the Selective search algorithm which takes around 2 seconds per image and runs on CPU computation. The Faster R-CNN [3] paper fixes this by using another convolutional network (the RPN) to generate the region proposals. This not only brings down the region proposal time from 2s to 10ms per image but also allows the region proposal stage to share layers with the following detection stages, causing an overall improvement in feature representation. In the rest of the article, “Faster R-CNN” usually refers to a detection pipeline that uses \n","the RPN as a region proposal algorithm, and Fast R-CNN as a detector network. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","\n","Faster-RCNN \n","Our object detection system, called Faster R-CNN, is com- posed of two modules. The first module is a deep fully convo- lutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared. We will be first implementing a simple CNN model with 11 layers. \n","Region Proposal Networks \n","RPN is small neural network sliding on the last feature map of the convolution layers and predict wether there is an object or not and also predict the bounding box of those objects. \n","\n","III. DATASET\n"," \n","We will be working on a healthcare related dataset and the aim here is to solve a Blood Cell Detection problem. Our task is to detect all the Red Blood Cells (RBCs), White Blood Cells (WBCs), and Platelets in each image taken via microscopic image readings. Below is a sample of what our final predictions should look like: \n","\n","We have three kind of labels : \n","    • RBC (Red Blood Cell)  \n","    • WBC (White Blood Cell) • Platelets  The reason for choosing this dataset is that the density of RBCs, WBCs and Platelets in our blood stream provides a lot of information about the immune system and hemoglobin. This can help us potentially identify whether a person is healthy or not, and if any discrepancy is found in their blood, actions can be taken quickly to diagnose that.  Manually looking at the sample via a microscope is a tedious process. And this is where Deep Learning models play such a vital role. They can classify and detect the blood cells from microscopic images with impressive precision. \n","\n","V. RESULTS \n","So our model has been trained and the weights are set. Keras_frcnn makes the predictions for the new images and saves them in a new folder. We just have to run the test_frcnn.py file to save the images. Below are a few examples of the predictions I got after implementing Faster R-CNN: \n","\n","VI. CONCLUSION \n","R-CNN algorithms have truly been a game-changer for object detection tasks. There has suddenly been a spike in recent years in the amount of computer vision applications being created, and R-CNN is at the heart of most of them. Keras_frcnn proved to be an excellent library for object detection, and in the upcoming labs, we will focus on more advanced techniques like Y\n"]}]},{"cell_type":"markdown","metadata":{"id":"oF4-PqF0Fl7R"},"source":["## **Prefix**\n","\n","We can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there.\n","\n","We can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, we can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup.\n","\n","Other optional-but-helpful parameters for `gpt2.generate`:\n","\n","*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n","* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n","* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, we want to set `top_k=40`)\n","* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n","* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n","*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."]},{"cell_type":"code","metadata":{"id":"8DKMc0fiej4N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638294468602,"user_tz":-330,"elapsed":11895,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}},"outputId":"4a0a4cad-2c9d-4613-f822-a2d4d5dc2e04"},"source":["gpt2.generate(sess,\n","              length=250,\n","              temperature=0.7,\n","              prefix=\"Neural Network\",\n","              nsamples=5,\n","              batch_size=5\n","              )"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Neural Network (CNN) and Fast R-CNN. \n","In the R-CNN family of papers, the evolution between versions was usually in terms of computational efficiency (integrating the different training stages), reduction in test time, and improvement in performance (mAP). These net- works usually consist of — a) A region proposal algorithm to generate “bounding boxes” or locations of possible objects in the image; b) A feature generation stage to obtain features of these objects, usually using a CNN; c) A classification layer to predict which class this object belongs to; and d) A regression layer to make the coordinates of the object bounding box more precise. \n","The only stand-alone portion of the network left in Fast R-CNN was the region proposal algorithm. Both R-CNN and Fast R-CNN use CPU based region proposal algorithms, Eg- the Selective search algorithm which takes around 2 seconds per image and runs on CPU computation. The Faster R-CNN [3] paper fixes this by using another convolutional network (the RPN) to generate the region proposals. This not only brings down the region proposal time from 2s to 10ms per image but also allows the region proposal stage to share layers\n","====================\n","Neural Network (RPN) is com- posed of two modules. The first module is a deep fully convo- lutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared. We will be first implementing a simple CNN model with 11 layers. \n","Region Proposal Networks \n","RPN is small neural network sliding on the last feature map of the convolution layers and predict wether there is an object or not and also predict the bounding box of those objects. \n","\n","III. DATASET\n"," \n","We will be working on a healthcare related dataset and the aim here is to solve a Blood Cell Detection problem. Our task is to detect all the Red Blood Cells (RBCs), White Blood Cells (WBCs\n","====================\n","Neural Network (CNN) to train and test your model on 40,000 images to categorize a tube that is poorly placed. The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning. \n","\n","Index Terms—Hyperparameter, Tensorflow, Keras,CNN I. INTRODUCTION \n","Serious complications can occur as a result of mal- positioned lines and tubes in patients. Doctors and nurses fre- quently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity. \n","Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube mal-positioning into the airways has been reported in up to 3% of cases, with up to 40%\n","====================\n","Neural Network (RPN) for object detection has been proposed. The process of selecting the right set of nodes for a detection network is called classification. This classification system requires that some features be taken into account but does not require that others be taken. For a review of the different neural networks, check out these summaries: Regions with CNN (RPN) and Fast R-CNN. \n","In the RPN world, the evolution between versions was usually in terms of computational efficiency (integrating the different training stages), reduction in test time, and improvement in performance (mAP). These net- works usually consist of — a) A region proposal algorithm to generate “bounding boxes” or locations of possible objects in the image; b) A feature generation stage to obtain features of these objects, usually using a CNN; c) A classification layer to predict which class this object belongs to; and d) A regression layer to make the coordinates of the object bounding box more precise. \n","The only stand-alone portion of the network left in Fast R-CNN was the region proposal algorithm. Both R-CNN and Fast R-CNN use CPU based region proposal algorithms, Eg- the Selective search algorithm which takes around 2 seconds\n","====================\n","Neural Network (CNN) to generate the region proposals. This not only brings down the region proposal time from 2s to 10ms per image but also allows the region proposal stage to share layers with the following detection stages, causing an overall improvement in feature representation. In the rest of the article, “Faster R-CNN” usually refers to a detection pipeline that uses \n","the RPN as a region proposal algorithm, and Fast R-CNN as a detector network. [1], [2], [3], [4], [5], [6] \n","\n","II. METHOD \n","\n","Faster-RCNN \n","Our object detection system, called Faster R-CNN, is com- posed of two modules. The first module is a deep fully convo- lutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of\n","====================\n"]}]},{"cell_type":"markdown","metadata":{"id":"zjjEN2Tafhl2"},"source":["For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n","\n","You can rerun the cells as many times as you want for even more generated texts!"]},{"cell_type":"code","metadata":{"id":"Fa6p6arifSL0","executionInfo":{"status":"ok","timestamp":1638295577537,"user_tz":-330,"elapsed":124149,"user":{"displayName":"Black Arrow","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10859633162653787298"}}},"source":["gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n","\n","gpt2.generate_to_file(sess,\n","                      destination_path=gen_file,\n","                      length=500,\n","                      temperature=0.7,\n","                      nsamples=100,\n","                      batch_size=20\n","                      )"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0rRtQjo7D9rh"},"source":["## **INFERENCE**\n","The above examples were generated using the 124M model finetuned on a custom dataset. If the machine is capable we may try the larger models for some state-of-the-art text generation. Probably try a larger model 355M, 774M, 1558M\n","and finetune it for better ressults."]},{"cell_type":"code","metadata":{"id":"NSIq9NYlBLxb"},"source":[""],"execution_count":null,"outputs":[]}]}